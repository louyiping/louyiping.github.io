<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="MachineLearning," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="前段时间面试，准备的都是机器学习相关的岗位，虽然最后没能找到算法岗位，但还是抽时间将之前的一些东西整理了一下。 深度学习 BP算法：基于梯度下降策略，以目标的负梯度方向对参数进行调整。 防止过拟合：1.早停：训练集误差降低但验证集误差升高，停止训练。2.正则化。CNN它的特殊性体现在两个方面，一方面它的神经元间的连接是非全连接的， 另一方面同一层中某些神经元之间的连接的权重是共享的（即相同的）。降">
<meta name="keywords" content="MachineLearning">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习复习汇总">
<meta property="og:url" content="http://yoursite.com/2017/06/03/机器学习复习汇总/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="前段时间面试，准备的都是机器学习相关的岗位，虽然最后没能找到算法岗位，但还是抽时间将之前的一些东西整理了一下。 深度学习 BP算法：基于梯度下降策略，以目标的负梯度方向对参数进行调整。 防止过拟合：1.早停：训练集误差降低但验证集误差升高，停止训练。2.正则化。CNN它的特殊性体现在两个方面，一方面它的神经元间的连接是非全连接的， 另一方面同一层中某些神经元之间的连接的权重是共享的（即相同的）。降">
<meta property="og:image" content="http://img.blog.csdn.net/20160314132318542?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="http://img.blog.csdn.net/20160314132637422?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="http://img.blog.csdn.net/20160314133019676?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/42741-b9a16a53d58ca2b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/42741-96b387f711d1d12c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/42741-7fa07e640593f930.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/42741-d88caa3c4faf5353.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/42741-4c9186bf786063d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://pic2.zhimg.com/70/v2-db2872a90bf616921ee4beedb3f9e669_b.jpg">
<meta property="og:image" content="http://images2015.cnblogs.com/blog/927391/201607/927391-20160716145131217-650617034.png">
<meta property="og:image" content="http://images2015.cnblogs.com/blog/927391/201607/927391-20160716145206701-383430284.png">
<meta property="og:image" content="http://images2015.cnblogs.com/blog/927391/201607/927391-20160717142500264-1717908455.png">
<meta property="og:image" content="http://img.my.csdn.net/uploads/201302/25/1361786433_5005.jpg">
<meta property="og:image" content="http://note.youdao.com/yws/public/resource/b2e9f0fbb24624336266398224e5fcaa/xmlnote/090902115D14475EA50377469C29AE30/336">
<meta property="og:image" content="http://note.youdao.com/yws/public/resource/b2e9f0fbb24624336266398224e5fcaa/xmlnote/C531FA39280A4A98BD4C55DF4B5FCC6C/334">
<meta property="og:updated_time" content="2017-06-04T14:46:14.255Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习复习汇总">
<meta name="twitter:description" content="前段时间面试，准备的都是机器学习相关的岗位，虽然最后没能找到算法岗位，但还是抽时间将之前的一些东西整理了一下。 深度学习 BP算法：基于梯度下降策略，以目标的负梯度方向对参数进行调整。 防止过拟合：1.早停：训练集误差降低但验证集误差升高，停止训练。2.正则化。CNN它的特殊性体现在两个方面，一方面它的神经元间的连接是非全连接的， 另一方面同一层中某些神经元之间的连接的权重是共享的（即相同的）。降">
<meta name="twitter:image" content="http://img.blog.csdn.net/20160314132318542?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/06/03/机器学习复习汇总/"/>





  <title>机器学习复习汇总 | Hexo</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  















  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/06/03/机器学习复习汇总/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习复习汇总</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-03T13:39:26+08:00">
                2017-06-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MachineLearning/" itemprop="url" rel="index">
                    <span itemprop="name">MachineLearning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>前段时间面试，准备的都是机器学习相关的岗位，虽然最后没能找到算法岗位，但还是抽时间将之前的一些东西整理了一下。</p>
<h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><ul>
<li>BP算法：基于梯度下降策略，以目标的负梯度方向对参数进行调整。</li>
<li>防止过拟合：1.早停：训练集误差降低但验证集误差升高，停止训练。2.正则化。<h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3>它的特殊性体现在两个方面，一方面它的神经元间的连接是非全连接的， 另一方面同一层中某些神经元之间的连接的权重是共享的（即相同的）。<br>降低了网络模型的复杂度（对于很难学习的深层结构来说，这是非常重要的），减少了权值的数量。<br>是一种局部连接网络。<br>卷积网络是为识别二维形状而特殊设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他形式的变形具有高度不变性。 这些良好的性能是网络在有监督方式下学会的，网络的结构主要有稀疏连接和权值共享两个特点，包括如下形式的约束：</li>
</ul>
<ol>
<li>特征提取。每一个神经元从上一层的局部接受域得到突触输人，因而迫使它提取局部特征。一旦一个特征被提取出来， 只要它相对于其他特征的位置被近似地保留下来，它的精确位置就变得没有那么重要了。</li>
<li>特征映射。网络的每一个计算层都是由多个特征映射组成的，每个特征映射都是平面形式的。平面中单独的神经元在约束下共享 相同的突触权值集，这种结构形式具有如下的有益效果：a.平移不变性。b.自由参数数量的缩减(通过权值共享实现)。<br>3、子抽样。每个卷积层后面跟着一个实现局部平均和子抽样的计算层，由此特征映射的分辨率降低。这种操作具有使特征映射的输出对平移和其他 形式的变形的敏感度下降的作用。</li>
</ol>
<p>卷积神经网络是一个多层的神经网络，每层由多个二维平面组成，而每个平面由多个独立神经元组成。</p>
<h4 id="什麽造成梯度消失问题"><a href="#什麽造成梯度消失问题" class="headerlink" title="什麽造成梯度消失问题?"></a>什麽造成梯度消失问题?</h4><p>神经网络的训练中，通过改变神经元的权重，使网络的输出值尽可能逼近标签以降低误差值，训练普遍使用BP算法，核心思想是，计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，进行权值的迭代。梯度消失会造成权值更新缓慢，模型训练难度增加。造成梯度消失的一个原因是，许多激活函数将输出值挤压在很小的区间内，在激活函数两端较大范围的定义域内梯度为0。造成学习停止。<br>每一层所产生的误差会逐渐向之前的层次传播，而各层的权重根据梯度下降算法不断地优化。总之，反向传播算法的核心就是梯度下降 + 链式法则求偏导，虽然看起来很繁琐并且计算复杂度有点高，但是实际上BP算法的精确性和易用性是很难被其他算法替代的。</p>
<h4 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h4><p><img src="http://img.blog.csdn.net/20160314132318542?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="image"><br><img src="http://img.blog.csdn.net/20160314132637422?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="image"></p>
<p>这样，我们就可以根据最后求出的误差来对权重进行更新，这种误差反向传递的方式就是反向传播算法的精髓所在。处理完输出层神经元，我们再来看一下输出层之前的一层神经元的权值更新，我们定义表示连接前一层神经元和后一层神经元的权值。</p>
<p><img src="http://img.blog.csdn.net/20160314133019676?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="image"></p>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p><img src="http://upload-images.jianshu.io/upload_images/42741-b9a16a53d58ca2b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p>
<p>在我们LSTM中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为忘记门层完成。该门会读取h_{t-1}和x<em>t，输出一个在 0 到1之间的数值给每个在细胞状态C</em>{t-1}中的数字。1表示“完全保留”，0表示“完全舍弃”。<br>让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。<br><img src="http://upload-images.jianshu.io/upload_images/42741-96b387f711d1d12c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"><br>下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量，\tilde{C}_t，会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。<br>在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。<br><img src="http://upload-images.jianshu.io/upload_images/42741-7fa07e640593f930.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"><br>现在是更新旧细胞状态的时间了，C_{t-1}更新为 C_t。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。我们把旧状态与 f_t 相乘，丢弃掉我们确定需要丢弃的信息。接着加上 i_t * \tilde{C}_t。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。<br>在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。<br><img src="http://upload-images.jianshu.io/upload_images/42741-d88caa3c4faf5353.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"><br>最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个sigmoid层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和sigmoid门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。<br>在语言模型的例子中，因为他就看到了一个代词，可能需要输出与一个动词相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。<br><img src="http://upload-images.jianshu.io/upload_images/42741-4c9186bf786063d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p>
<ul>
<li>输入门i_t：控制有多少信息可以流入memory cell（第四个式子c_t）。</li>
<li>遗忘门f_t：控制有多少上一时刻的memory cell中的信息可以累积到当前时刻的memory cell中。</li>
<li>输出门o_t：控制有多少当前时刻的memory cell中的信息可以流入当前隐藏状态h_t中。</li>
</ul>
<p>注：gates并不提供额外信息，gates只是起到限制信息的量的作用。因为gates起到的是过滤器作用，所以所用的激活函数是sigmoid而不是tanh。</p>
<h3 id="鞍点"><a href="#鞍点" class="headerlink" title="鞍点"></a>鞍点</h3><ol>
<li>鞍点也是驻点，鞍点处的梯度为零，在一定范围内沿梯度下降会自然往鞍点走（吸引域）。在上面双曲面的情形下，这个吸引域占整体搜索空间的1/2，但是在高维的复杂情形其比例可能会很高。</li>
<li>优化过程不是卡在鞍点不动了(像人们以为的局部极值那样)，而是在接近鞍点的过程中梯度越来越小，越来越接近于零，于是变动的幅度越来越小，loss看起来就像是卡住了。</li>
<li>直观来看增加一些扰动，从下降到鞍点的路径上跳出去就能绕过鞍点。但在高维的情形，这个必然到达鞍点的吸引域范围可能是非常大的，不能只靠低维的直观来推断。此外，即便从一个鞍点跳过(跳出了吸引域)，这个跳出来的部分很可能是另一个鞍点的吸引域——鞍点的数量(可能)是指数级的。<br><img src="http://pic2.zhimg.com/70/v2-db2872a90bf616921ee4beedb3f9e669_b.jpg" alt="image"></li>
</ol>
<h1 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h1><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><ul>
<li>集成：同质（基学习器）或异质（组件学习器）<br>好的集成：个体学习器要有准确性和多样性。不然可能还有负作用。</li>
<li>两类：1.个体学习器间存在强依赖关系，串行生成的（Boosting）2.不存在强依赖关系，并行(Bagging,RF)</li>
<li>集成策略：1.平均法2.投票法3.学习法</li>
<li>boosting算法：“重赋值法”，“重采样法”，主要关注降低偏差。</li>
<li>bagging与RF：使用相互有交叠的采样子集。主要关注降低方差。</li>
<li>bagging：训练一个bagging集成与直接使用个体学习器的复杂度同阶，未采样的数据可用作验证集。</li>
<li>随机森林是bagging的拓展，RF在以决策树为基学习器构建bagging集成的基础上，进一步在决策树的训练过程中引入了随机的属性选择。RF训练效率优于bagging（属性子集）</li>
<li>随机森林分类效果（错误率）与两个因素有关：</li>
<li><ol>
<li>森林中任意两棵树的相关性：相关性越大，错误率越大；</li>
</ol>
</li>
<li><ol>
<li>森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。</li>
</ol>
</li>
</ul>
<p><img src="http://images2015.cnblogs.com/blog/927391/201607/927391-20160716145131217-650617034.png" alt="image"></p>
<p>bagging：</p>
<p><img src="http://images2015.cnblogs.com/blog/927391/201607/927391-20160716145206701-383430284.png" alt="image"></p>
<p>根据上式我们可以看到，整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似。同时，整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。但是，模型的准确度一定会无限逼近于1吗？并不一定，当基模型数增加到一定程度时，方差公式第二项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。另外，在此我们还知道了为什么bagging中的基模型一定要为强模型，否则就会导致整体模型的偏差度低，即准确度低。</p>
<p>Random Forest是典型的基于bagging框架的模型，其在bagging的基础上，进一步降低了模型的方差。Random Fores中基模型是树模型，在树的内部节点分裂过程中，不再是将所有特征，而是随机抽样一部分特征纳入分裂的候选项。这样一来，基模型之间的相关性降低，从而在方差公式中，第一项显著减少，第二项稍微增加，整体方差仍是减少。<br>boosting:</p>
<p><img src="http://images2015.cnblogs.com/blog/927391/201607/927391-20160717142500264-1717908455.png" alt="image"></p>
<p>通过观察整体方差的表达式，我们容易发现，若基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，boosting框架中的基模型必须为弱模型。</p>
<p>因为基模型为弱模型，导致了每个基模型的准确度都不是很高（因为其在训练集上的准确度不高）。随着基模型数的增多，整体模型的期望值增加，更接近真实值，因此，整体模型的准确度提高。但是准确度一定会无限逼近于1吗？仍然并不一定，因为训练过程中准确度的提高的主要功臣是整体模型在训练集上的准确度提高，而随着训练的进行，整体模型的方差变大，导致防止过拟合的能力变弱，最终导致了准确度反而有所下降。</p>
<p>基于boosting框架的Gradient Tree Boosting模型中基模型也为树模型，同Random Forrest，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。</p>
<h4 id="Dropout原理分析"><a href="#Dropout原理分析" class="headerlink" title="Dropout原理分析"></a>Dropout原理分析</h4><p>Dropout可以看做是一种模型平均，所谓模型平均，顾名思义，就是把来自不同模型的估计或者预测通过一定的权重平均起来，在一些文献中也称为模型组合，它一般包括组合估计和组合预测。</p>
<p>Dropout中哪里体现了“不同模型”；这个奥秘就是我们随机选择忽略隐层节点，在每个批次的训练过程中，由于每次随机忽略的隐层节点都不同，这样就使每次训练的网络都是不一样的，每次训练都可以单做一个“新”的模型；此外，隐含节点都是以一定概率随机出现，因此不能保证每2个隐含节点每次都同时出现，这样权值的更新不再依赖于有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况。</p>
<p>这样dropout过程就是一个非常有效的神经网络模型平均方法，通过训练大量的不同的网络，来平均预测概率。不同的模型在不同的训练集上训练（每个批次的训练数据都是随机选择），最后在每个模型用相同的权重来“融合”</p>
<h1 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>核函数的作用就是隐含着一个从低维空间到高维空间的映射，而这个映射可以把低维空间中线性不可分的两类点变成线性可分的。</p>
<h3 id="核函数的选取"><a href="#核函数的选取" class="headerlink" title="核函数的选取"></a>核函数的选取</h3><p><img src="http://img.my.csdn.net/uploads/201302/25/1361786433_5005.jpg" alt="image"></p>
<ol>
<li>如果如果特征数远远大于样本数的情况下,使用线性核就可以了.</li>
<li>如果特征数和样本数都很大,例如文档分类,一般使用线性核, LIBLINEAR比LIBSVM速度要快很多.</li>
<li>如果特征数远小于样本数,这种情况一般使用RBF.但是如果一定要用线性核,则选择LIBLINEAR较好。</li>
</ol>
<h4 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h4><p>SMO算法是支持向量机学习的一种快速方法，其特点是不断地将原二次规划问题分解为只有两个变量的二次规划子问题，并对子问题进行解析求解，直到所有变量满足KKT条件为止。这样通过启发式的方法得到原二次规划问题的最优解。因为子问题有解析解，所有每次计算子问题都很快，虽然计算子问题次数很多，但在总体上还是很高效的。</p>
<h1 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h1><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><ol>
<li>先统计出每天每小时每条线路的客流量等需要用到的基本信息，乘客卡种（学生卡，老年卡，普通卡等）：</li>
<li>统计出每个线路每小时各个卡种的人群流量，再进行除法求出每个线路每小时一个乘客持有某种卡的概率，这相当于归一化。因为乘客流量可能层次不齐，从个位数到几千，这样可以在同一度量下处理。</li>
<li>再把天气条件量化，比如气候就根据从晴到阴的程度做编号再量化。</li>
<li>再求出该日期是星期几，因为双休日与工作日差别较大。</li>
<li>制定一系列超参数，用sklearn中的GridSearchCV找最佳参数是多少，再用最佳参数训练数据。<br>（参数包括：learning_rate，loss种类，max_depth，n_estimators）</li>
</ol>
<h5 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h5><p>周几，时间(hour)，最低温度，最高温度，温差，量化后的气候，乘客卡种。</p>
<h2 id="GBDT原理："><a href="#GBDT原理：" class="headerlink" title="GBDT原理："></a>GBDT原理：</h2><p>利用梯度下降法的近似方法，关键是利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值，拟合一个回归树。</p>
<ol>
<li>计算损失函数的负梯度在当前模型的值，将它作为残差的估计。对于平方损失函数，它就是通常所说的残差；对于一般损失函数，它就是残差的近似值。</li>
<li>然后对残差拟合一个回归树，得到新增的树的叶节点的区域，来拟合残差的近似值。</li>
<li>使得损失函数极小化，求出叶节点区域的值。</li>
<li>更新回归树。<br><img src="http://note.youdao.com/yws/public/resource/b2e9f0fbb24624336266398224e5fcaa/xmlnote/090902115D14475EA50377469C29AE30/336" alt="image"></li>
</ol>
<h2 id="GBDT参数："><a href="#GBDT参数：" class="headerlink" title="GBDT参数："></a>GBDT参数：</h2><h5 id="GBDT类库boosting框架参数"><a href="#GBDT类库boosting框架参数" class="headerlink" title="GBDT类库boosting框架参数"></a>GBDT类库boosting框架参数</h5><ol>
<li>n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是100。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。</li>
<li>learning_rate: 即每个弱学习器的权重缩减系数νν，也称作步长，在原理篇的正则化章节我们也讲到了，加上了正则化项，我们的强学习器的迭代公式为fk(x)=fk−1(x)+νhk(x)fk(x)=fk−1(x)+νhk(x)。νν的取值范围为0&lt;ν≤10&lt;ν≤1。对于同样的训练集拟合效果，较小的νν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的νν开始调参，默认是1。</li>
<li>subsample: 即我们在原理篇的正则化章节讲到的子采样，取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。</li>
<li>init: 即我们的初始化的时候的弱学习器，拟合对应原理篇里面的f0(x)f0(x)，如果不输入，则用训练集样本来做样本集的初始化分类回归预测。否则用init参数提供的学习器做初始化分类回归预测。一般用在我们对数据有先验知识，或者之前做过一些拟合的时候，如果没有的话就不用管这个参数了。</li>
<li>loss: 即我们GBDT算法中的损失函数。分类模型和回归模型的损失函数是不一样的。对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。在原理篇中对这些分类损失函数有详细的介绍。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。一般来说，如果数据的噪音点不多，用默认的均方差”ls”比较好。如果是噪音点较多，则推荐用抗噪音的损失函数”huber”。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。</li>
<li>alpha：这个参数只有GradientBoostingRegressor有，当我们使用Huber损失”huber”和分位数损失“quantile”时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。<h5 id="GBDT类库弱学习器参数（CART决策树类）："><a href="#GBDT类库弱学习器参数（CART决策树类）：" class="headerlink" title="GBDT类库弱学习器参数（CART决策树类）："></a>GBDT类库弱学习器参数（CART决策树类）：</h5></li>
<li>划分时考虑的最大特征数max_features: 可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数；如果是”log2”意味着划分时最多考虑log2Nlog2N个特征；如果是”sqrt”或者”auto”意味着划分时最多考虑N−−√N个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的”None”就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。</li>
<li>决策树最大深度max_depth: 默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。</li>
<li>内部节点再划分所需最小样本数min_samples_split: 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</li>
<li>叶子节点最少样本数min_samples_leaf: 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</li>
<li>叶子节点最小的样本权重和min_weight_fraction_leaf：这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。</li>
<li>最大叶子节点数max_leaf_nodes: 通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。</li>
<li>节点划分最小不纯度min_impurity_split:  这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。</li>
</ol>
<h2 id="GBDT如何调参？"><a href="#GBDT如何调参？" class="headerlink" title="GBDT如何调参？"></a>GBDT如何调参？</h2><ol>
<li>GBDT中决策树的叶子数，通常GBDT中决策树的叶子数控制在4-8之间，效果比较好</li>
<li>GBDT正则化，涉及到过拟合问题，正则化减小模型复杂度，防止过拟合</li>
<li>迭代次数 M太小，学习效果会有提升的空间，M太大导致过拟合，通常使用CV来检测M是否为有效地迭代次数</li>
<li>Shrinkage，<br><img src="http://note.youdao.com/yws/public/resource/b2e9f0fbb24624336266398224e5fcaa/xmlnote/C531FA39280A4A98BD4C55DF4B5FCC6C/334" alt="image"><br>通常，比较小的学习率通常来带来不错的模型泛化能力，但是训练与预测时间会增加</li>
<li>限制每个叶子的数据数量：类似于决策树的过拟合方法，如果某个条件下的数据个数小于我们规定的值，那么就不会被分支，减少模型复杂性</li>
</ol>
<h6 id="正则化的目的：防止过拟合！"><a href="#正则化的目的：防止过拟合！" class="headerlink" title="正则化的目的：防止过拟合！"></a>正则化的目的：防止过拟合！</h6><h6 id="正则化的本质：约束（限制）要优化的参数。"><a href="#正则化的本质：约束（限制）要优化的参数。" class="headerlink" title="正则化的本质：约束（限制）要优化的参数。"></a>正则化的本质：约束（限制）要优化的参数。</h6><h2 id="GBDT正则化方法："><a href="#GBDT正则化方法：" class="headerlink" title="GBDT正则化方法："></a>GBDT正则化方法：</h2><ol>
<li>Early Stopping<br>选择一部分样本作为验证集，在迭代拟合训练集的过程中，如果模型在验证集里错误率不再下降，就停止训练，也就是说控制迭代的轮数（树的个数）</li>
<li>Shrinkage<br>Shrinkage就是将每棵树的输出结果乘一个因子。导致各个树的残差是渐变的而不是陡变的。νν和迭代轮数M(树个数)是一个tradeoff，推荐的是νν值设置小一点(如0.1)，而M设置大一些。这样一般能有比较好的准确率，代价是训练时间变长(与M成比例)。<br>learning_rate=0.1, n_estimators=100</li>
<li>Subsampling<br>Subsampling其实源于bootstrap averaging(bagging)思想，GBDT里的做法是在每一轮建树时，样本是从训练集合中无放回随机抽样的ηη部分，典型的ηη值是0.5。这样做既能对模型起正则作用，也能减少计算时间。也可以进行列抽样。训练耗时长。</li>
</ol>
<h2 id="XGBoost的区别："><a href="#XGBoost的区别：" class="headerlink" title="XGBoost的区别："></a>XGBoost的区别：</h2><ol>
<li>gboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。</li>
<li>Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）<br>列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。</li>
<li>对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。</li>
<li>xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</li>
</ol>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/MachineLearning/" rel="tag"># MachineLearning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/12/29/JVM/" rel="prev" title="JVM复习">
                JVM复习 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="John Doe" />
          <p class="site-author-name" itemprop="name">John Doe</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#深度学习"><span class="nav-number">1.</span> <span class="nav-text">深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CNN"><span class="nav-number">1.0.1.</span> <span class="nav-text">CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#什麽造成梯度消失问题"><span class="nav-number">1.0.1.1.</span> <span class="nav-text">什麽造成梯度消失问题?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#推导过程"><span class="nav-number">1.0.1.2.</span> <span class="nav-text">推导过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LSTM"><span class="nav-number">1.0.1.3.</span> <span class="nav-text">LSTM</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#鞍点"><span class="nav-number">1.0.2.</span> <span class="nav-text">鞍点</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#集成学习"><span class="nav-number">2.</span> <span class="nav-text">集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#概念"><span class="nav-number">2.0.0.1.</span> <span class="nav-text">概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dropout原理分析"><span class="nav-number">2.0.0.2.</span> <span class="nav-text">Dropout原理分析</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SVM"><span class="nav-number">3.</span> <span class="nav-text">SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#核函数"><span class="nav-number">3.1.</span> <span class="nav-text">核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#核函数的选取"><span class="nav-number">3.1.1.</span> <span class="nav-text">核函数的选取</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SMO算法"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">SMO算法</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GBDT"><span class="nav-number">4.</span> <span class="nav-text">GBDT</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#流程"><span class="nav-number">4.1.</span> <span class="nav-text">流程</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#特征"><span class="nav-number">4.1.0.0.1.</span> <span class="nav-text">特征</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GBDT原理："><span class="nav-number">4.2.</span> <span class="nav-text">GBDT原理：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GBDT参数："><span class="nav-number">4.3.</span> <span class="nav-text">GBDT参数：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#GBDT类库boosting框架参数"><span class="nav-number">4.3.0.0.1.</span> <span class="nav-text">GBDT类库boosting框架参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GBDT类库弱学习器参数（CART决策树类）："><span class="nav-number">4.3.0.0.2.</span> <span class="nav-text">GBDT类库弱学习器参数（CART决策树类）：</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GBDT如何调参？"><span class="nav-number">4.4.</span> <span class="nav-text">GBDT如何调参？</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#正则化的目的：防止过拟合！"><span class="nav-number">4.4.0.0.0.1.</span> <span class="nav-text">正则化的目的：防止过拟合！</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#正则化的本质：约束（限制）要优化的参数。"><span class="nav-number">4.4.0.0.0.2.</span> <span class="nav-text">正则化的本质：约束（限制）要优化的参数。</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GBDT正则化方法："><span class="nav-number">4.5.</span> <span class="nav-text">GBDT正则化方法：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBoost的区别："><span class="nav-number">4.6.</span> <span class="nav-text">XGBoost的区别：</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  





  

  

  

  

  

  

</body>
</html>
